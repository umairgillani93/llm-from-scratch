Config = {
    "vocab_size": 50257, # Vocabulary size
    "ctx_len": 1024, # Context length
    "emb_dim": 768, # embedding dimension
    "num_heads": 12, # number of attention heads
    "num_layers": 12, # number of layers
    "dropout_rate": 0.1, # Dropout rate
    "qkv_bias": False # Query-Key-Value bias
    }
